{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from collections import OrderedDict\n",
    "\n",
    "import ROOT\n",
    "ROOT.gROOT.SetBatch(True)\n",
    "from ROOT import TCanvas, TFile, TProfile, TNtuple, TH1F, TH2F, TH1D, TH2D, THStack, TF1\n",
    "from ROOT import gBenchmark, gRandom, gSystem, Double, gPad, TFitResultPtr, TMath\n",
    "import root_numpy\n",
    "import psutil\n",
    "import pandas\n",
    "import math\n",
    "#matplotlib.use('agg')\n",
    "%matplotlib inline\n",
    "import matplotlib #as matplot\n",
    "print(matplotlib.__version__)\n",
    "#print(matplotlib.path)\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm as cm\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pylab\n",
    "import sklearn as sk\n",
    "print(sk.__version__)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "######################\n",
    "import keras as kr\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer, Input\n",
    "from keras.layers import Reshape, MaxPooling2D\n",
    "from keras.layers import Conv2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU, ELU, PReLU \n",
    "from keras.optimizers import Adamax\n",
    "from keras.optimizers import Nadam\n",
    "print(kr.__version__)\n",
    "#from keras import backend as K\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=32, \\\n",
    "                        inter_op_parallelism_threads=32, \\\n",
    "                        allow_soft_placement=True, \\\n",
    "                        device_count = {'CPU': 32}\n",
    "                       )\n",
    "session = tf.Session(config=config)\n",
    "#K.set_session(session)\n",
    "## it will issue a warning, just ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test that matplotlib imported ok\n",
    "x1, y1 = [-1, 12], [1, 4]\n",
    "x2, y2 = [1, 10], [3, 2]\n",
    "plt.plot(x1, y1, x2, y2, marker = 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel='2l_2tau_HH'\n",
    "\n",
    "startTime = datetime.now()\n",
    "execfile(\"../python/data_manager.py\") \n",
    "\n",
    "if channel=='2l_2tau_HH' : \n",
    "    execfile(\"../cards/info_2l_2tau_HH.py\") \n",
    "    execfile(\"../cards/auxiliary_functions_2l_2tau_HH.py\") ## Loading the functions and setup variables specific to 2l_2tau channels \n",
    "    execfile(\"../cards/NN_settings_2l_2tau_HH.py\") ## Loading the boolean settings specific to 2l_2tau channel\n",
    "    \n",
    "log_file_name=channel+\".log\"\n",
    "if 'evtLevelSUM_HH_2l_2tau_res' in bdtType:\n",
    "    file1_ = open(log_file_name, 'w+')\n",
    "else: \n",
    "    file1_ = open('roc.log','w+')    \n",
    "\n",
    "\n",
    "import shutil,subprocess\n",
    "proc=subprocess.Popen(['mkdir '+channel],shell=True,stdout=subprocess.PIPE)\n",
    "out = proc.stdout.read()    \n",
    "    \n",
    "    \n",
    "output = read_from(Bkg_mass_rand, tauID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"read from:\", output[\"inputPath\"])\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "\n",
    "if(TrainMode == 0): ## All Masses included in training \n",
    "    data=load_data_2017(\n",
    "            output[\"inputPath\"],\n",
    "            output[\"channelInTree\"],\n",
    "            trainVars(True),\n",
    "            [],\n",
    "            bdtType,\n",
    "            channel,\n",
    "            output[\"keys\"],\n",
    "            output[\"masses\"],\n",
    "            output[\"mass_randomization\"],\n",
    "            )\n",
    "    mass_list = output[\"masses\"]\n",
    "    test_masses = output[\"masses_test\"]\n",
    "elif(TrainMode == 1): ## Only Low Masses (<= 400 GeV) included in training                                                                                                                                                                                              \n",
    "        data=load_data_2017(\n",
    "            output[\"inputPath\"],\n",
    "            output[\"channelInTree\"],\n",
    "            trainVars(True),\n",
    "            [],\n",
    "            bdtType,\n",
    "            channel,\n",
    "            output[\"keys\"],\n",
    "            output[\"masses_low\"],\n",
    "            output[\"mass_randomization\"],\n",
    "            )\n",
    "        mass_list = output[\"masses_low\"]\n",
    "        test_masses = output[\"masses_test_low\"]\n",
    "elif(TrainMode == 2): ## Only High Masses (> 400 GeV) included in training                                                                                                                                                                                              \n",
    "        data=load_data_2017(\n",
    "            output[\"inputPath\"],\n",
    "            output[\"channelInTree\"],\n",
    "            trainVars(True),\n",
    "            [],\n",
    "            bdtType,\n",
    "            channel,\n",
    "            output[\"keys\"],\n",
    "            output[\"masses_high\"],\n",
    "            output[\"mass_randomization\"],\n",
    "            )\n",
    "        mass_list = output[\"masses_high\"]\n",
    "        test_masses = output[\"masses_test_high\"]\n",
    "else:        \n",
    "    data=load_data_2017(\n",
    "        output[\"inputPath\"], \n",
    "        output[\"channelInTree\"], \n",
    "        trainVars(True), \n",
    "        [], \n",
    "        bdtType, \n",
    "        channel,\n",
    "        output[\"keys\"], \n",
    "        output[\"masses\"],\n",
    "        output[\"mass_randomization\"]\n",
    "        ) # note: I had to add channel as argument of the function\n",
    "    mass_list = output[\"masses\"]\n",
    "    test_masses = output[\"masses_test\"]\n",
    "    \n",
    "    \n",
    "#data.dropna(subset=[\"totalWeight\"],inplace = True) ## Was used in the BDT code\n",
    "#data.fillna(0)                                     ## Was used in the BDT code\n",
    "\n",
    "print (len(data))\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotAll=False\n",
    "BDTvariables=trainVars(plotAll, variables, bdtType)\n",
    "\n",
    "import copy\n",
    "\n",
    "## Removing gen_mHH from the list of input variables                                                                                                                                                                                                                    \n",
    "BDTvariables_wo_gen_mHH = copy.deepcopy(BDTvariables)  ## Works                                                                                                                                                                                                         \n",
    "BDTvariables_wo_gen_mHH.remove(\"gen_mHH\")\n",
    "\n",
    "trainvar = variables\n",
    "\n",
    "if channel=='2l_2tau_HH' :\n",
    "    labelBKG = \"TT+DY+VV\"\n",
    "else:\n",
    "    labelBKG = \"TT+DY+VV+W\"\n",
    "    \n",
    "print(\"mass_list\", mass_list)\n",
    "print(\"test_masses\", test_masses)\n",
    "\n",
    "if((channel=='2l_2tau_HH') and do_ReweightVars):\n",
    "        DoFits = True\n",
    "        print DoFits\n",
    "        print(\"Perfoming Fits to TProfile plots for signal\")\n",
    "        ## --- Making TProfile plots with fits (Signal) --- ###\n",
    "        MakeTProfile_New(channel, data, BDTvariables_wo_gen_mHH, 1, DoFits, \"before\", TrainMode, mass_list)\n",
    "else:\n",
    "        DoFits = False\n",
    "        print DoFits\n",
    "        print(\"Not Perfoming Fits to TProfile plots for signal\")\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "if((channel=='2l_2tau_HH') and do_2l_2tau_diagnostics == True):  \n",
    "    ## --- Making TProfile plots w/o fits (background) --- ###                                                                                                                                           \n",
    "    MakeTProfile_New(channel, data, BDTvariables_wo_gen_mHH, 0, False, \"before\", TrainMode, mass_list)        \n",
    "\n",
    "    ## --- Making 1D Histo plots (background) --- ###                                                                                                                                                                                                                   \n",
    "    MakeHisto1D_New(channel, data, BDTvariables, \"before\")\n",
    "\n",
    "    ## --- Making 1D THStack plots (background) --- ###                                                                                                                                                                                                                 \n",
    "    MakeTHStack_New(channel, data, BDTvariables, \"before\")\n",
    "else:\n",
    "    print(\"No plots will be made for 2l_2tau diagnostics\")  \n",
    "\n",
    "if((channel=='2l_2tau_HH') and do_ReweightVars):\n",
    "    ## ----- SCALING I/P VAR.S IN DATA USING THE FITS DONE ABOVE---- ###                                                                                                                                                                                            \n",
    "    ReweightDataframe_New(data, channel, BDTvariables_wo_gen_mHH, mass_list)\n",
    "else:\n",
    "    print(\"No plots and fits will be made for 2l_2tau diagnostics\")\n",
    "    \n",
    "    \n",
    "weights=\"evtWeight\"\n",
    "\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "data[\"weight_cx\"] = data[weights]\n",
    "data[\"weight_train\"] = data[weights]\n",
    "data[\"NN_output\"] = 1.0\n",
    "\n",
    "print (\"separate datasets odd/even\")\n",
    "data_even = data.loc[(data[\"event\"].values % 2 == 0) ]\n",
    "data_odd = data.loc[~(data[\"event\"].values % 2 == 0) ]\n",
    "\n",
    "order_train = [data_odd, data_even]\n",
    "order_train_name = [\"odd\",\"even\"]\n",
    "\n",
    "\n",
    "print (\"balance datasets by even/odd chunck\")\n",
    "for data_do in order_train :\n",
    "    #### Normalization by cross section                                                                                                                                                                                                                                   \n",
    "    for wei in [\"weight_cx\", \"weight_train\"] :\n",
    "        if 'SUM_HH' in bdtType :\n",
    "            data_do.loc[(data_do['key'].isin(['TTTo2L2Nu','TTToSemiLeptonic'])), [wei]]              *= output[\"TTdatacard\"]/data_do.loc[(data_do['key'].isin(['TTTo2L2Nu','TTToSemiLeptonic'])), weights].sum()\n",
    "            data_do.loc[(data_do['key']=='DY'), [wei]]                            *= output[\"DYdatacard\"]/data_do.loc[(data_do['key']=='DY'), weights].sum()\n",
    "            if \"evtLevelSUM_HH_bb1l_res\" in bdtType :\n",
    "                data_do.loc[(data_do['key']=='W'), [wei]]                         *= Wdatacard/data_do.loc[(data_do['key']=='W')].sum() ## Saswati check please !!!                                                                                                       \n",
    "        if \"evtLevelSUM_HH_2l_2tau_res\" in bdtType :\n",
    "               #data_do.loc[(data_do['key']=='TTZJets'), [wei]]                       *= output[\"TTZdatacard\"]/data_do.loc[(data_do['key']=='TTZJets'), weights].sum() ## TTZJets                                                                                          \n",
    "               #data_do.loc[(data_do['key']=='TTWJets'), [wei]]                       *= output[\"TTWdatacard\"]/data_do.loc[(data_do['key']=='TTWJets'), weights].sum() ## TTWJets + TTWW                                                                                   \n",
    "               data_do.loc[(data_do['key']=='ZZ'), [wei]]                            *= output[\"ZZdatacard\"]/data_do.loc[(data_do['key']=='ZZ'), weights].sum() ## ZZ +ZZZ                                                                                                \n",
    "               data_do.loc[(data_do['key']=='WZ'), [wei]]                            *= output[\"WZdatacard\"]/data_do.loc[(data_do['key']=='WZ'), weights].sum() ## WZ + WZZ_4F                                                                                            \n",
    "               data_do.loc[(data_do['key']=='WW'), [wei]]                            *= output[\"WWdatacard\"]/data_do.loc[(data_do['key']=='WW'), weights].sum() ## WW + WWZ + WWW_4F                                                                                      \n",
    "               #data_do.loc[(data_do['key']=='VH'), [wei]]                        *= output[\"VHdatacard\"]/data_do.loc[(data_do['key']=='VH'), weights].sum() # consider removing                                                                                          \n",
    "               #data_do.loc[(data_do['key']=='TTH'), [wei]]                       *= output[\"TTHdatacard\"]/data_do.loc[(data_do['key']=='TTH'), weights].sum() # consider removing    \n",
    "    ### Normalize sig/BKG and do table of nevents/mass\n",
    "    for mass in output[\"masses\"] :\n",
    "        data_do.loc[(data_do['target']==1) & (data_do[\"gen_mHH\"] == mass),\"weight_train\"] *= 10000./data_do.loc[(data_do['target']==1) & (data_do[\"gen_mHH\"]== mass), \"weight_train\"].sum() ## Changed from 1000\n",
    "        data_do.loc[(data_do['target']==0) & (data_do[\"gen_mHH\"] == mass),\"weight_train\"] *= 10000./data_do.loc[(data_do['target']==0) & (data_do[\"gen_mHH\"]== mass), \"weight_train\"].sum() ## Changed from 1000 \n",
    "    print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "\n",
    "    print (\"training statistics by mass\")\n",
    "    for mass in output[\"masses\"] :\n",
    "        print (\n",
    "               str(mass)+\": sig = \"+\\\n",
    "               str(len(data_do.loc[(data['target']==1) & (data_do[\"gen_mHH\"] == mass),[\"weight_train\"]]))+\\\n",
    "               \" BKG = \"+str(len(data_do.loc[(data['target']==0) & (data_do[\"gen_mHH\"] == mass),[\"weight_train\"]]))\n",
    "              )\n",
    "\n",
    "    print (\"\\n norm by mass - test\")\n",
    "    for mass in output[\"masses\"] :\n",
    "        print (\n",
    "               str(mass)+\": sig = \"+\\\n",
    "               str(data_do.loc[(data_do['target']==1) & (data_do[\"gen_mHH\"] == mass),\"weight_train\"].sum())+\\\n",
    "               \" BKG = \"+str(data_do.loc[(data_do['target']==0) & (data_do[\"gen_mHH\"] == mass),\"weight_train\"].sum())\n",
    "              )\n",
    "\n",
    "if((channel=='2l_2tau_HH') and do_2l_2tau_diagnostics == True):        \n",
    "    ## ----Merging the odd and even data-sets ---------------------------------------####                                                                                                                                                                               \n",
    "    ## ---(reweighting the merged dataframe by 0.5 as it is derived from 2 halves) ---###                                                                                                                                                                               \n",
    "    data_odd_copy = data_odd.copy(deep=True) ## Making sure we do not alter the halves used for roc curves later                                                                                                                                                        \n",
    "    data_even_copy = data_even.copy(deep=True) ## Making sure we do not alter the halves used for roc curves later                                                                                                                                                      \n",
    "    data_do = data_odd_copy.append(data_even_copy, ignore_index=True)\n",
    "    data_do.loc[data_do['target']==0, [weights]] *= 0.5\n",
    "    data_do.loc[data_do['target']==1, [weights]] *= 0.5\n",
    "    label = \"after\"\n",
    "\n",
    "    ## --- Making TProfile plots w/o fitting (Signal) --- ###                                                                                                                                                                                                           \n",
    "    MakeTProfile_New(channel, data, BDTvariables_wo_gen_mHH, 1, False, label, TrainMode, mass_list)\n",
    "\n",
    "    ## --- Making TProfile plots (background) --- ###                                                                                                                                                                                                                   \n",
    "    MakeTProfile_New(channel, data, BDTvariables_wo_gen_mHH, 0, False, label, TrainMode, mass_list)\n",
    "\n",
    "    ## --- Making 1D Histo plots (background) --- ###                                                                                                                                                                                                                   \n",
    "    MakeHisto1D_New(channel, data,  BDTvariables, label)\n",
    "\n",
    "    ## --- Making 1D THStack plots (background) --- ###                                                                                                                                                                                                                 \n",
    "    MakeTHStack_New(channel, data, BDTvariables, label)\n",
    "else:\n",
    "    print(\"No plots will be made for 2l_2tau diagnostics\")    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Check of the resulting weights - the sizes of the training weight\n",
    "\"\"\"\n",
    "fig, ax = plt.subplots(figsize=(4, 4))\n",
    "if(channel=='2l_2tau_HH'):\n",
    "    keysToBKG = ['WW', 'WZ', 'ZZ', 'DY', 'TTTo2L2Nu', 'TTToSemiLeptonic'] # 'VH', 'TTH', 'TTToHadronic', 'TTZJets', 'TTWJets' \n",
    "else:\n",
    "    keysToBKG = ['WW', 'WZ', 'ZZ', 'DY', 'TTTo2L2Nu', 'TTToSemiLeptonic', 'W']\n",
    "    \n",
    "#colors = ['cyan','orange','k','r','green','magenta','b',]\n",
    "vars = [\"weight_train\"]#\"multitarget\"]\n",
    "\n",
    "for kk, key in enumerate(keysToBKG) :\n",
    "  for vv, var in enumerate(vars) : \n",
    "    ax.hist(\n",
    "        np.array(data.loc[(data['key']==key), var].values,dtype='float64'), # \n",
    "        weights=data.loc[(data['key']==key), \"evtWeight\"], # \"weight_train_cat\"\n",
    "        range=(-1.0,10.),bins=40, histtype='step', normed=True, lw=2, \n",
    "        label=key\n",
    "    )\n",
    "    ax.set_xlabel(var)\n",
    "ax.legend(loc=\"best\", title= channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the variables\n",
    "#trainvar = trainVars(False, \"testVars2\")\n",
    "#print trainvar\n",
    "trainvar = trainVars(False, variables, bdtType) ## = BDTvariables\n",
    "print trainvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Draw some plots on lists of variables for BKG\n",
    "\"\"\"\n",
    "#keysToBKG = ['TTZJets', 'WZ', 'ZZ', 'DY', 'VH', 'ttH', 'TTTo2L2Nu']\n",
    " \n",
    "## i try to do 3 X 3 plots (= enter up to nine entries in each sublist)\n",
    "## try to add strictly decreasing variables as first in each sublist, better for the legend positioning\n",
    "#listdraw = [\n",
    "#    [ 'tau1_pt', 'tau2_pt', 'mT_lep1', 'mT_lep2', 'met', 'm_ll', 'mTauTau', 'diHiggsVisMass', 'diHiggsMass' ], \n",
    "#    ['nBJet_medium', 'nElectron', 'max_tau_eta', 'max_lep_eta', 'gen_mHH'],\n",
    "#]\n",
    "\n",
    "if(channel=='2l_2tau_HH'):\n",
    "    listdraw = [\n",
    "        ['diHiggsMass', 'diHiggsVisMass', 'tau1_pt', 'nBJet_medium', 'nElectron', 'dr_lep_tau_min_SS', 'met_LD', 'tau2_pt', 'dr_lep_tau_min_OS'],\n",
    "        ['gen_mHH'],\n",
    "               ]\n",
    "else:\n",
    "    listdraw = [\n",
    "        ['diHiggsMass', 'diHiggsVisMass', 'tau1_pt', 'nBJet_medium', 'nElectron', 'dr_lep_tau_min_SS', 'met_LD', 'tau2_pt', 'dr_lep_tau_min_OS'],\n",
    "        ['gen_mHH'],\n",
    "               ]\n",
    "\n",
    "for featuresDraw in listdraw:\n",
    "    sizeArray=int(math.sqrt(len(featuresDraw))) if math.sqrt(len(featuresDraw)) % int(math.sqrt(len(featuresDraw))) == 0 else int(math.sqrt(len(featuresDraw)))+1\n",
    "    plt.figure(figsize=(4*sizeArray,4*sizeArray))\n",
    "    for n, feature in enumerate(featuresDraw) :\n",
    "        min_value, max_value = np.percentile(data[feature], [0.0, 99])\n",
    "        # fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        plt.subplot(sizeArray, sizeArray, n+1)\n",
    "        for kk, key in enumerate(keysToBKG) :\n",
    "            if 'TTZJets' in key or 'TTWJets' in key : linestyle = \"--\"\n",
    "            else :linestyle = \"-\"\n",
    "            plt.hist(\n",
    "            np.array(data.loc[(data['key']==key), feature].values,dtype='float64'), \n",
    "            weights=data.loc[(data['key']==key) , \"evtWeight\"], \n",
    "            range=(min_value, max_value), \n",
    "            bins=12, histtype='step', ls=linestyle, \n",
    "            normed=True, lw=2, #color=colors[kk],\n",
    "            label=key\n",
    "            )\n",
    "            plt.xlabel(feature)\n",
    "        if n == 0 : plt.legend(loc=\"upper right\", title= channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for featuresDraw in listdraw:\n",
    "    sizeArray=int(math.sqrt(len(featuresDraw))) if math.sqrt(len(featuresDraw)) % int(math.sqrt(len(featuresDraw))) == 0 else int(math.sqrt(len(featuresDraw)))+1\n",
    "    plt.figure(figsize=(4*sizeArray,4*sizeArray))\n",
    "    for n, feature in enumerate(featuresDraw) :\n",
    "        min_value, max_value = np.percentile(data[feature], [0.0, 99])\n",
    "        # fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        plt.subplot(sizeArray, sizeArray, n+1)\n",
    "        for mass in [300,400,700] :\n",
    "            plt.hist(\n",
    "            np.array(data.loc[(data[\"gen_mHH\"] == mass), feature].values,dtype='float64'), \n",
    "            weights=data.loc[(data[\"gen_mHH\"] == mass) , \"evtWeight\"], \n",
    "            range=(min_value, max_value), \n",
    "            bins=10, histtype='step', ls=linestyle, \n",
    "            normed=True, lw=2, #color=colors[kk],\n",
    "            label=\"mass = \"+str(mass)\n",
    "            )\n",
    "        plt.hist(\n",
    "        np.array(data.loc[(data['target']==0), feature].values,dtype='float64'), \n",
    "        weights=data.loc[(data['target']==0) , \"evtWeight\"], \n",
    "        range=(min_value, max_value), \n",
    "        bins=10, histtype='step', ls='--', \n",
    "        normed=True, lw=3, color='k',\n",
    "        label=\"BKG\"\n",
    "        )\n",
    "        plt.xlabel(feature)\n",
    "        if n == 0 : plt.legend(loc=\"upper right\", title= channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model -- for binary activation='sigmoid'\n",
    "nclasses = 1\n",
    "\n",
    "## ---- Set these values for NN hyper-parameters (for gen_mHH as Input Var.) --- ##                                                                                                                                                                                                                     \n",
    "EPOCHS=60 \n",
    "DROPOUT=0.05 # 0.01 is more optimum\n",
    "LR=0.00005\n",
    "SCHEDULE_DECAY=0.000005\n",
    "BATCH_SIZE=256 \n",
    "#EPOCHS=35\n",
    "#DROPOUT=0.01\n",
    "#LR=0.0005\n",
    "#SCHEDULE_DECAY=0.00005\n",
    "#BATCH_SIZE=256 ## 32,256\n",
    "##----------------------------------------------------------##       \n",
    "\n",
    "## ---- Set these values for NN hyper-parameters (w/o gen_mHH as Input Var.) --- ##                                                                                                                                                                                                                     \n",
    "EPOCHS2=35\n",
    "DROPOUT2=0.01\n",
    "LR2=0.0005\n",
    "SCHEDULE_DECAY2=0.00005\n",
    "BATCH_SIZE2=256 ## 32,256\n",
    "##----------------------------------------------------------##       \n",
    "\n",
    "\n",
    "hyppar=str(variables)+\"_epochs_\"+str(EPOCHS)+\"_dropout_\"+num_to_str(DROPOUT)+\"_lr_\"+num_to_str(LR)+\"_sch_decay_\"+num_to_str(SCHEDULE_DECAY)+\"_batch_size_\"+str(BATCH_SIZE)\n",
    "hyppar2=str(variables)+\"_wo_gen_mHH\"+\"_epochs_\"+str(EPOCHS2)+\"_dropout_\"+num_to_str(DROPOUT2)+\"_lr_\"+num_to_str(LR2)+\"_sch_decay_\"+num_to_str(SCHEDULE_DECAY2)+\"_batch_size_\"+str(BATCH_SIZE2)\n",
    "\n",
    "print hyppar\n",
    "print(\"DROPOUT\", DROPOUT)\n",
    "print(\"LR\", LR)\n",
    "print(\"SCHEDULE_DECAY\", SCHEDULE_DECAY)\n",
    "\n",
    "print hyppar2\n",
    "print(\"DROPOUT2\", DROPOUT2)\n",
    "print(\"LR2\", LR2)\n",
    "print(\"SCHEDULE_DECAY2\", SCHEDULE_DECAY2)\n",
    "\n",
    "\n",
    "features = trainvar # = BDTvariables\n",
    "features2 = BDTvariables_wo_gen_mHH\n",
    "\n",
    "def nn_model_binary():\n",
    "    \"create a model.\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2*len(features), input_dim=len(features), kernel_initializer='he_uniform')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(DROPOUT)) #0.1\n",
    "    for Nnodes in [8,8] :\n",
    "        model.add(Dense(Nnodes, kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(DROPOUT)) #0.1\n",
    "    model.add(Dense(nclasses, activation='sigmoid'))\n",
    "    model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=Nadam(lr=LR, schedule_decay=SCHEDULE_DECAY), ##  lr=0.0005, sch_dec.=0.00005   # , beta_1 = 0.95, beta_2 = 0.999\n",
    "    metrics=['accuracy'], \n",
    "    )\n",
    "    return model\n",
    "\n",
    "def nn_model_binary_wo_gen_mHH():\n",
    "    \"create a model.\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(2*len(features2), input_dim=len(features2), kernel_initializer='he_uniform')) \n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(DROPOUT2)) #0.1\n",
    "    for Nnodes in [8,8] :\n",
    "        model.add(Dense(Nnodes, kernel_initializer='he_uniform'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(PReLU())\n",
    "        model.add(Dropout(DROPOUT2)) #0.1\n",
    "    model.add(Dense(nclasses, activation='sigmoid'))\n",
    "    model.compile(\n",
    "    loss='binary_crossentropy', \n",
    "    optimizer=Nadam(lr=LR2, schedule_decay=SCHEDULE_DECAY2), ##  lr=0.0005, sch_dec.=0.00005   # , beta_1 = 0.95, beta_2 = 0.999\n",
    "    metrics=['accuracy'], \n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_binary().summary()\n",
    "nn_model_binary_wo_gen_mHH().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Port Keras Framework into SK-Learn\n",
    "# https://stackoverflow.com/questions/39467496/error-when-using-keras-sk-learn-api\n",
    "k_model_binary  = KerasClassifier(\n",
    "    build_fn=nn_model_binary, ## For gen_mHH as input Var.\n",
    "    epochs=EPOCHS,            ## For gen_mHH as input Var.\n",
    "    batch_size=BATCH_SIZE,    ## For gen_mHH as input Var.\n",
    "    verbose=2                 ## For gen_mHH as input Var.\n",
    "    #build_fn=nn_model_binary_wo_gen_mHH, ## w/o gen_mHH as input Var.\n",
    "    #epochs=EPOCHS2,                      ## w/o gen_mHH as input Var. \n",
    "    #batch_size=BATCH_SIZE2,              ## w/o gen_mHH as input Var.\n",
    "    #verbose=2                            ## w/o gen_mHH as input Var.\n",
    ")\n",
    "\n",
    "k_model_binary2  = KerasClassifier(\n",
    "    build_fn=nn_model_binary, ## For gen_mHH as input Var.\n",
    "    epochs=EPOCHS,            ## For gen_mHH as input Var. \n",
    "    batch_size=BATCH_SIZE,    ## For gen_mHH as input Var.\n",
    "    verbose=2                 ## For gen_mHH as input Var.\n",
    "    #build_fn=nn_model_binary_wo_gen_mHH, ## w/o gen_mHH as input Var.\n",
    "    #epochs=EPOCHS2,                      ## w/o gen_mHH as input Var. \n",
    "    #batch_size=BATCH_SIZE2,              ## w/o gen_mHH as input Var.\n",
    "    #verbose=2                            ## w/o gen_mHH as input Var.\n",
    ")\n",
    "\n",
    "print(\"data_odd[features].values\", data_odd[features].values)\n",
    "print(\"data_odd[target].values\", data_odd['target'].values)\n",
    "print(\"sample_weight=data_odd[weight_train].values\", data_odd[\"weight_train\"].values)\n",
    "\n",
    "history = k_model_binary.fit(\n",
    "    data_odd[features].values, ## For gen_mHH as input Var.\n",
    "    #data_odd[features2].values, ## w/o gen_mHH as input Var.\n",
    "    data_odd['target'].values,\n",
    "    sample_weight=data_odd[\"weight_train\"].values,\n",
    "    validation_data=(\n",
    "        data_even[features].values, ## For gen_mHH as input Var.\n",
    "        #data_even[features2].values, ## w/o gen_mHH as input Var.\n",
    "        data_even['target'].values, \n",
    "        data_even[\"weight_train\"].values\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "history2 = k_model_binary2.fit(\n",
    "    data_even[features].values, ## For gen_mHH as input Var.\n",
    "    #data_even[features2].values, ## w/o gen_mHH as input Var.\n",
    "    data_even['target'].values,\n",
    "    sample_weight=data_even[\"weight_train\"].values,\n",
    "    validation_data=(\n",
    "        data_odd[features].values, ## For gen_mHH as input Var.\n",
    "        #data_odd[features2].values,  ## w/o gen_mHH as input Var. \n",
    "        data_odd['target'].values, \n",
    "        data_odd[\"weight_train\"].values\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "overtraining test\n",
    "\"\"\"\n",
    "# Extract number of run epochs from the training history\n",
    "epochs = range(1, len(history.history[\"loss\"])+1)\n",
    "plt.figure(figsize=(9, 4))\n",
    "#fig = plt.figure(figsize=(4, 4))\n",
    "#plt.subplot(1, 2, 1)\n",
    "plt.subplot(2, 2, 1)\n",
    "# Extract loss on training and validation dataset and plot them together\n",
    "plt.plot(epochs, history.history[\"loss\"], \"o-\", label=\"Training (Odd)\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], \"o-\", label=\"Validation (Even)\")\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "#plt.xlim(0,40)\n",
    "plt.ylim(0.2,0.8)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\");\n",
    "\n",
    "#plt.subplot(1, 2, 2)\n",
    "plt.subplot(2, 2, 2)\n",
    "#fig = plt.figure(figsize=(4, 4))\n",
    "# Extract loss on training and validation dataset and plot them together\n",
    "plt.plot(epochs, history.history[\"acc\"], \"o-\", label=\"Training (Odd)\")\n",
    "plt.plot(epochs, history.history[\"val_acc\"], \"o-\", label=\"Validation (Even)\")\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.4,0.9)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "# Extract loss on training and validation dataset and plot them together\n",
    "plt.plot(epochs, history2.history[\"loss\"], \"o-\", label=\"Training (Even)\")\n",
    "plt.plot(epochs, history2.history[\"val_loss\"], \"o-\", label=\"Validation (Odd)\")\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Loss\")\n",
    "plt.yscale(\"log\")\n",
    "#plt.xlim(0,40)\n",
    "plt.ylim(0.2,0.8)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\");\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "#fig = plt.figure(figsize=(4, 4))\n",
    "# Extract loss on training and validation dataset and plot them together\n",
    "plt.plot(epochs, history2.history[\"acc\"], \"o-\", label=\"Training (Even)\")\n",
    "plt.plot(epochs, history2.history[\"val_acc\"], \"o-\", label=\"Validation (Odd)\")\n",
    "plt.xlabel(\"Epochs\"), plt.ylabel(\"Accuracy\")\n",
    "plt.yscale(\"log\")\n",
    "plt.ylim(0.4,0.9)\n",
    "plt.grid()\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "filename1=\"{}/loss_and_acc_{}_Log.pdf\".format(channel, hyppar) ## For gen_mHH as input Var.\n",
    "#filename1=\"{}/loss_and_acc_{}_Log.pdf\".format(channel, hyppar2) ## w/o gen_mHH as input Var.\n",
    "plt.savefig(filename1);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/ram/.local/lib/python2.7/site-packages')\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "#from eli5.sklearn import permutation_importance\n",
    "\"\"\"\n",
    "to calculate variables importance, it takes time and it is not completelly 'enlightant',\n",
    "do not do all the time.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "perm = PermutationImportance(k_model_binary, random_state=1).fit( # , scoring=\"f1_samples\"\n",
    "    data_odd[features].values, ## For gen_mHH as input Var.\n",
    "    #data_odd[features2].values, ## w/o gen_mHH as input Var.\n",
    "    data_odd['target'].values,\n",
    "    sample_weight=data_odd[\"weight_train\"].values\n",
    ")\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "eli5.show_weights(perm, feature_names = data_odd[features].columns.tolist(), top=len(features)) ## For gen_mHH as input Var.\n",
    "#eli5.show_weights(perm, feature_names = data_odd[features2].columns.tolist(), top=len(features2)) ## w/o gen_mHH as input Var.\n",
    "\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "perm2 = PermutationImportance(k_model_binary2, random_state=1).fit( # , scoring=\"f1_samples\"\n",
    "    data_even[features].values, ## For gen_mHH as input Var.\n",
    "    #data_even[features2].values, ## w/o gen_mHH as input Var.\n",
    "    data_even['target'].values,\n",
    "    sample_weight=data_even[\"weight_train\"].values\n",
    ")\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "eli5.show_weights(perm2, feature_names = data_even[features].columns.tolist(), top=len(features)) ## For gen_mHH as input Var.\n",
    "#eli5.show_weights(perm2, feature_names = data_even[features2].columns.tolist(), top=len(features2)) ## w/o gen_mHH as input Var."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Calculate the output in all dataset \n",
    "-- to pass to the training/test\n",
    "\"\"\"\n",
    "\n",
    "## For gen_mHH as input Var.\n",
    "data_odd[\"NN_output\"]  = k_model_binary.predict_proba(data_odd[features].values, verbose=1)[:, 1]\n",
    "data_even[\"NN_output\"] = k_model_binary2.predict_proba(data_even[features].values, verbose=1)[:, 1]\n",
    "\n",
    "## w/o gen_mHH as input Var.\n",
    "#data_odd[\"NN_output\"]  = k_model_binary.predict_proba(data_odd[features2].values, verbose=1)[:, 1]\n",
    "#data_even[\"NN_output\"] = k_model_binary2.predict_proba(data_even[features2].values, verbose=1)[:, 1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hist_params = {'normed': True, 'bins': 10 , 'histtype':'step'}\n",
    "target = 'target'\n",
    "plt.clf()\n",
    "\n",
    "plt.figure('XGB',figsize=(6, 6))\n",
    "\n",
    "values, bins, _ = plt.hist(\n",
    "    data_odd.loc[data_odd.target.values == 1, \"NN_output\"].values , \n",
    "    weights=data_odd.loc[data_odd.target.values == 1, \"weight_cx\"].values,\n",
    "    label=\"sig odd\", color='r', range=(0,1), **hist_params\n",
    "    )\n",
    "values, bins, _ = plt.hist(\n",
    "    data_odd.loc[data_odd.target.values == 0, \"NN_output\"].values , \n",
    "    weights=data_odd.loc[data_odd.target.values == 0, \"weight_cx\"].values,\n",
    "    label=\"BKG odd\", color='g', range=(0,1), **hist_params\n",
    "    )\n",
    "\n",
    "values, bins, _ = plt.hist(\n",
    "    data_even.loc[data_even.target.values == 1, \"NN_output\"].values , \n",
    "    weights=(data_even.loc[data_even.target.values == 1, \"weight_cx\"].values),\n",
    "    label=\"sig even\", color='r', ls='--', range=(0,1), **hist_params)\n",
    "values, bins, _ = plt.hist(\n",
    "    data_even.loc[data_even.target.values == 0, \"NN_output\"].values , \n",
    "    weights=(data_even.loc[data_even.target.values == 0, \"weight_cx\"].values),\n",
    "    label=\"BKG even\", color='g', ls='--', range=(0,1), **hist_params)\n",
    "\n",
    "#plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='upper center')\n",
    "#filename2=\"{}/NN_Output_{}.pdf\".format(channel, hyppar) ## For gen_mHH as input Var.\n",
    "filename2=\"{}/NN_Output_{}.pdf\".format(channel, hyppar2) ## w/o gen_mHH as input Var.\n",
    "plt.savefig(filename2);\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# ROC Curve\n",
    "styleline = ['-', '--', '-.', ':']\n",
    "colors_mass = ['m', 'b', 'k', 'r', 'g',  'y', 'c', ]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sl = 0\n",
    "\n",
    "for dd, data_do in  enumerate(order_train) :\n",
    "        if dd == 0 : val_data = 1\n",
    "        else : val_data = 0\n",
    "        #print data_do[\"weight_cx\"].astype(np.float64)  \n",
    "        #print data_do.NN_output.values\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "                data_do[\"target\"].astype(np.bool), \n",
    "                data_do.NN_output.values, \n",
    "                sample_weight=data_do[\"weight_cx\"].astype(np.float64)\n",
    "        )\n",
    "        train_auc = auc(fpr, tpr, reorder = True)\n",
    "        print(\"train set auc \" + str(train_auc))\n",
    "        fprt, tprt, thresholds = roc_curve(\n",
    "            order_train[val_data][\"target\"].astype(np.bool), \n",
    "            order_train[val_data].NN_output.values, \n",
    "            sample_weight=(order_train[val_data][\"weight_cx\"].astype(np.float64))\n",
    "        )\n",
    "        test_auct = auc(fprt, tprt, reorder = True)\n",
    "        print(\"test set auc \" + str(test_auct))\n",
    "        ax.plot(\n",
    "            fpr, tpr,\n",
    "            lw = 2, linestyle = styleline[dd + dd*1], color = colors_mass[1],\n",
    "            label = order_train_name[dd] + ' train (area = %0.3f)'%(train_auc) + \")\"\n",
    "            )\n",
    "        sl += 1\n",
    "        ax.plot(\n",
    "            fprt, tprt,\n",
    "            lw = 2, linestyle = styleline[dd + 1 + + dd*1], color = colors_mass[2],\n",
    "            label = order_train_name[dd] + ' test (area = %0.3f)'%(test_auct) + \")\"\n",
    "            )\n",
    "        sl += 1\n",
    "ax.set_ylim([0.0,1.0])\n",
    "ax.set_xlim([0.0,1.0])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend(loc=\"lower right\", fontsize = 'small')\n",
    "ax.grid()\n",
    "\n",
    "#filename2a=\"{}/ROC_{}.pdf\".format(channel, hyppar) ## For gen_mHH as input Var.\n",
    "filename2a=\"{}/ROC_{}.pdf\".format(channel, hyppar2) ## w/o gen_mHH as input Var.\n",
    "plt.savefig(filename2a);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "## classifier plot by mass\n",
    "hist_params = {'normed': True, 'bins': 8 , 'histtype':'step', \"lw\": 2}\n",
    "plt.clf()\n",
    "colorcold = ['g', 'r', 'y']\n",
    "colorhot = ['b', 'magenta', 'orange']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "for mm, mass in enumerate(output[\"masses_test\"]) :\n",
    "    y_pred = data_even.loc[(data_even.target.values == 0) & (data_even[\"gen_mHH\"] == mass), \"NN_output\"].values\n",
    "    y_predS = data_even.loc[(data_even.target.values == 1) & (data_even[\"gen_mHH\"] == mass), \"NN_output\"].values\n",
    "    y_pred_train = data_odd.loc[(data_odd.target.values == 0) & (data_odd[\"gen_mHH\"] == mass), \"NN_output\"].values\n",
    "    y_predS_train = data_odd.loc[(data_odd.target.values == 1) & (data_odd[\"gen_mHH\"] == mass), \"NN_output\"].values\n",
    "    dict_plot = [\n",
    "       [y_pred, \"-\", colorhot[mm],  str(mass)+\" GeV test BKG\"],\n",
    "       [y_predS, \"-\", colorcold[mm], str(mass)+\" GeV test signal\"],\n",
    "       [y_pred_train, \"--\", colorhot[mm], str(mass)+\" GeV train BKG\" ],\n",
    "       [y_predS_train, \"--\", colorcold[mm],      str(mass)+\" GeV train signal\"]\n",
    "    ]\n",
    "    for item in dict_plot :\n",
    "        values1, bins, _ = ax.hist(\n",
    "            item[0],\n",
    "            ls=item[1], color = item[2],\n",
    "            label=item[3],\n",
    "            range=(0,1),\n",
    "            **hist_params\n",
    "            )\n",
    "        normed = sum(y_pred)\n",
    "        mid = 0.5*(bins[1:] + bins[:-1])\n",
    "        err=np.sqrt(values1*normed)/normed # denominator is because plot is normalized\n",
    "        plt.errorbar(mid, values1, yerr=err, fmt='none', color= item[2], ecolor= item[2], edgecolor=item[2], lw=2)\n",
    "#plt.xscale('log')\n",
    "#plt.yscale('log')\n",
    "ax.legend(loc='upper center', title=\"by mass \", fontsize = 'small')\n",
    "#nameout = channel+'/'+bdtType+'_'+trainvar+'_'+str(len(trainVars(False)))+'_'+hyppar+'_mass_'+ str(mass)+'_XGBclassifier.pdf'\n",
    "\n",
    "filename3=\"{}/NN_Output_by_mass_{}.pdf\".format(channel, hyppar) ## For gen_mHH as input Var.\n",
    "#filename3=\"{}/NN_Output_by_mass_{}.pdf\".format(channel, hyppar2) ## w/o gen_mHH as input Var.\n",
    "plt.savefig(filename3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# by mass ROC\n",
    "styleline = ['-', '--', '-.', ':']\n",
    "colors_mass = ['m', 'b', 'k', 'r', 'g',  'y', 'c', ]\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "sl = 0\n",
    "\n",
    "\n",
    "for mm, mass in enumerate(output[\"masses_test\"]) :\n",
    "    for dd, data_do in  enumerate(order_train) :\n",
    "        if dd == 0 : val_data = 1\n",
    "        else : val_data = 0\n",
    "        fpr, tpr, thresholds = roc_curve(\n",
    "            data_do.loc[(data_do[\"gen_mHH\"] == mass), \"target\"].astype(np.bool),\n",
    "            data_do.loc[(data_do[\"gen_mHH\"] == mass), \"NN_output\"].values,\n",
    "            sample_weight=(data_do.loc[(data_do[\"gen_mHH\"].astype(np.int) == int(mass)), \"weight_cx\"].astype(np.float64))\n",
    "        )\n",
    "        train_auc = auc(fpr, tpr, reorder = True)\n",
    "        print(\"train set auc \" + str(train_auc) + \" (mass = \" + str(mass) + \")\")\n",
    "        fprt, tprt, thresholds = roc_curve(\n",
    "            order_train[val_data].loc[(order_train[val_data][\"gen_mHH\"].astype(np.int) == int(mass)), target].astype(np.bool), \n",
    "            order_train[val_data].loc[(order_train[val_data][\"gen_mHH\"] == mass), \"NN_output\"].values, #proba[:,1],\n",
    "            sample_weight=(order_train[val_data].loc[(order_train[val_data][\"gen_mHH\"].astype(np.int) == int(mass)), \"weight_cx\"].astype(np.float64))\n",
    "        )\n",
    "        test_auct = auc(fprt, tprt, reorder = True)\n",
    "        print(\"test set auc \" + str(test_auct) + \" (mass = \" + str(mass) + \")\")\n",
    "        ax.plot(\n",
    "            fpr, tpr,\n",
    "            lw = 2, linestyle = styleline[dd + dd*1], color = colors_mass[mm],\n",
    "            label = order_train_name[dd] + ' train (area = %0.3f)'%(train_auc) + \" (mass = \" + str(mass) + \")\"\n",
    "            )\n",
    "        sl += 1\n",
    "        ax.plot(\n",
    "            fprt, tprt,\n",
    "            lw = 2, linestyle = styleline[dd + 1 + + dd*1], color = colors_mass[mm],\n",
    "            label = order_train_name[dd] + ' test (area = %0.3f)'%(test_auct) + \" (mass = \" + str(mass) + \")\"\n",
    "            )\n",
    "        sl += 1\n",
    "ax.set_ylim([0.0,1.0])\n",
    "ax.set_xlim([0.0,1.0])\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.legend(loc=\"lower right\", fontsize = 'small')\n",
    "ax.grid()\n",
    "\n",
    "filename4=\"{}/ROC_by_mass_{}.pdf\".format(channel, hyppar) ## For gen_mHH as input Var.\n",
    "#filename4=\"{}/ROC_by_mass_{}.pdf\".format(channel, hyppar2) ## w/o gen_mHH as input Var.\n",
    "plt.savefig(filename4);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "output a training and export to .pb (to be used on cpp)\n",
    "\"\"\"\n",
    "print (\"Date: \", time.asctime( time.localtime(time.time()) ))\n",
    "nameout = \"model_erase\"\n",
    "\n",
    "out = k_model_binary.model.save(\"test_\"+nameout+\"_\"+hyppar+\"_Odd.hdf5\")\n",
    "file = open(nameout+\"_variables.log\",\"w\")\n",
    "file.write(str(features)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "out = k_model_binary2.model.save(\"test_\"+nameout+\"_\"+hyppar+\"_Even.hdf5\")\n",
    "file = open(nameout+\"_variables.log\",\"w\")\n",
    "file.write(str(features)+\"\\n\")\n",
    "file.close()\n",
    "\n",
    "\n",
    "## This bellow does not work, easier to be done on the command line,\n",
    "## in any case the bellow is a template how to run it\n",
    "##!python ../test/convert_hdf5_2_pb.py --input \"test_\"+nameout+\".hdf5\" --output \"test_\"+nameout+\".pb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluating the NN training model here\n",
    "Using randomly split halves of a copy of the original dataframe\n",
    "\"\"\"\n",
    "\n",
    "data_copy = data.copy(deep=True)\n",
    "data_copy_reduced = data_copy.drop(columns=['NN_output'])\n",
    "data_train, data_test = train_test_split(data_copy_reduced, test_size=0.5, random_state=42)\n",
    "#print(\"data_test\", data_test)\n",
    "\n",
    "\n",
    "hdf5_file_name_Odd = \"test_\"+nameout+\"_\"+hyppar+\"_Odd.hdf5\"\n",
    "hdf5_file_name_Even = \"test_\"+nameout+\"_\"+hyppar+\"_Even.hdf5\"\n",
    "\n",
    "from keras.models import load_model\n",
    "k_model_loaded = load_model(hdf5_file_name_Odd)\n",
    "results = k_model_binary.model.evaluate(data_test[features].values, data_test['target'].values, batch_size=256, verbose=1, sample_weight=data_test['weight_cx'].values) ## Since Odd train, Even test\n",
    "print('test loss, test acc:', results)\n",
    "\n",
    "k_model_loaded2 = load_model(hdf5_file_name_Even)\n",
    "results2 = k_model_binary2.model.evaluate(data_train[features].values, data_train['target'].values, batch_size=256, verbose=1, sample_weight=data_train['weight_cx'].values) ## Since Odd train, Even test\n",
    "print('test2 loss, test2 acc:', results2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the next do correlation matrices with variables\n",
    "import seaborn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for target in [0,1] :\n",
    "    corr_mat = data.loc[(data['target']==0), features].astype(float).corr() #\n",
    "    fig, ax = plt.subplots(figsize=(20, 12)) \n",
    "    seaborn.heatmap(corr_mat, square=True, ax=ax, vmin=-1., vmax=1.)\n",
    "    if(target == 0) : filename4=\"{}/Signal_Correl_{}.pdf\".format(channel, hyppar)\n",
    "    else: filename4=\"{}/Background_Correl_{}.pdf\".format(channel, hyppar)   \n",
    "    plt.savefig(filename4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
